{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Realtime face recognition Code and weights are mainly from <a href=\"https://github.com/davidsandberg/facenet\">David Sandberg GitHub</a>\n",
    "<br>\n",
    "\n",
    "<table align=\"left\">\n",
    "  <thead>\n",
    "    <tr>\n",
    "      <th><p style=\"text-align:center;\">Environments</p></th>\n",
    "      <th><p style=\"text-align:center;\">Frameworks</th>\n",
    "      <th><p style=\"text-align:center;\">Mian Packages</th>\n",
    "      <th><p style=\"text-align:center;\">Notes</th>\n",
    "    </tr>\n",
    "  </thead>\n",
    "  <tbody>\n",
    "    <tr>\n",
    "      <td><p style=\"text-align:center;\">Python 3.7.7</td>\n",
    "      <td><p style=\"text-align:center;\">Tensorflow 1.15</td>\n",
    "      <td><p style=\"text-align:center;\">numpy, etc.,</td>\n",
    "      <td><p style=\"text-align:center;\">Suggest to creat you env through Anacond etc.,<br>due to the automatic installation of lots of packages<br>建議採用Anaconda建立環境, 可節省諸多package安裝過程</td>\n",
    "    </tr>\n",
    "    <tr>\n",
    "      <td><p style=\"text-align:center;\"> </td>\n",
    "      <td><p style=\"text-align:center;\"> </td>\n",
    "      <td><p style=\"text-align:center;\">OpenCV '4.4.0'</td>\n",
    "      <td><p style=\"text-align:center;\">Suggest to install the package by .whl file<br>建議下載whl檔案安裝, 透過Anaconda pip安裝的版本無法正常結束影片</td>\n",
    "    </tr>\n",
    "      <tr>\n",
    "      <td><p style=\"text-align:center;\"> </td>\n",
    "      <td><p style=\"text-align:center;\"> </td>\n",
    "      <td><p style=\"text-align:center;\"> </td>\n",
    "      <td><p style=\"text-align:center;\">we will use pretrained weights from David to run the model<br>我們會用David Sandberg已經學習好的參數(det1/det2/det3)來跑模型</td>\n",
    "    </tr>\n",
    "  </tbody>\n",
    "</table>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\user\\anaconda3\\envs\\py37_tf\\lib\\site-packages\\tensorflow\\python\\framework\\dtypes.py:516: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint8 = np.dtype([(\"qint8\", np.int8, 1)])\n",
      "C:\\Users\\user\\anaconda3\\envs\\py37_tf\\lib\\site-packages\\tensorflow\\python\\framework\\dtypes.py:517: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint8 = np.dtype([(\"quint8\", np.uint8, 1)])\n",
      "C:\\Users\\user\\anaconda3\\envs\\py37_tf\\lib\\site-packages\\tensorflow\\python\\framework\\dtypes.py:518: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint16 = np.dtype([(\"qint16\", np.int16, 1)])\n",
      "C:\\Users\\user\\anaconda3\\envs\\py37_tf\\lib\\site-packages\\tensorflow\\python\\framework\\dtypes.py:519: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint16 = np.dtype([(\"quint16\", np.uint16, 1)])\n",
      "C:\\Users\\user\\anaconda3\\envs\\py37_tf\\lib\\site-packages\\tensorflow\\python\\framework\\dtypes.py:520: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint32 = np.dtype([(\"qint32\", np.int32, 1)])\n",
      "C:\\Users\\user\\anaconda3\\envs\\py37_tf\\lib\\site-packages\\tensorflow\\python\\framework\\dtypes.py:525: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  np_resource = np.dtype([(\"resource\", np.ubyte, 1)])\n",
      "C:\\Users\\user\\anaconda3\\envs\\py37_tf\\lib\\site-packages\\tensorboard\\compat\\tensorflow_stub\\dtypes.py:541: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint8 = np.dtype([(\"qint8\", np.int8, 1)])\n",
      "C:\\Users\\user\\anaconda3\\envs\\py37_tf\\lib\\site-packages\\tensorboard\\compat\\tensorflow_stub\\dtypes.py:542: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint8 = np.dtype([(\"quint8\", np.uint8, 1)])\n",
      "C:\\Users\\user\\anaconda3\\envs\\py37_tf\\lib\\site-packages\\tensorboard\\compat\\tensorflow_stub\\dtypes.py:543: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint16 = np.dtype([(\"qint16\", np.int16, 1)])\n",
      "C:\\Users\\user\\anaconda3\\envs\\py37_tf\\lib\\site-packages\\tensorboard\\compat\\tensorflow_stub\\dtypes.py:544: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint16 = np.dtype([(\"quint16\", np.uint16, 1)])\n",
      "C:\\Users\\user\\anaconda3\\envs\\py37_tf\\lib\\site-packages\\tensorboard\\compat\\tensorflow_stub\\dtypes.py:545: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint32 = np.dtype([(\"qint32\", np.int32, 1)])\n",
      "C:\\Users\\user\\anaconda3\\envs\\py37_tf\\lib\\site-packages\\tensorboard\\compat\\tensorflow_stub\\dtypes.py:550: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  np_resource = np.dtype([(\"resource\", np.ubyte, 1)])\n"
     ]
    }
   ],
   "source": [
    "# MIT License\n",
    "# \n",
    "# Copyright (c) 2016 David Sandberg\n",
    "# \n",
    "# Permission is hereby granted, free of charge, to any person obtaining a copy\n",
    "# of this software and associated documentation files (the \"Software\"), to dealr\n",
    "# in the Software without restriction, including without limitation the rights\n",
    "# to use, copy, modify, merge, publish, distribute, sublicense, and/or sell\n",
    "# copies of the Software, and to permit persons to whom the Software is\n",
    "# furnished to do so, subject to the following conditions:\n",
    "# \n",
    "# The above copyright notice and this permission notice shall be included in all\n",
    "# copies or substantial portions of the Software.\n",
    "# \n",
    "# THE SOFTWARE IS PROVIDED \"AS IS\", WITHOUT WARRANTY OF ANY KIND, EXPRESS OR\n",
    "# IMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY,\n",
    "# FITNESS FOR A PARTICULAR PURPOSE AND NONINFRINGEMENT. IN NO EVENT SHALL THE\n",
    "# AUTHORS OR COPYRIGHT HOLDERS BE LIABLE FOR ANY CLAIM, DAMAGES OR OTHER \n",
    "# LIABILITY, WHETHER IN AN ACTION OF CONTRACT, TORT OR OTHERWISE, ARISING FROM,\n",
    "# OUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR OTHER DEALINGS IN THE\n",
    "# SOFTWARE.\n",
    "\n",
    "from __future__ import absolute_import\n",
    "from __future__ import division\n",
    "from __future__ import print_function\n",
    "from six import string_types, iteritems\n",
    "\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "#from math import floor\n",
    "import cv2\n",
    "import os\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from scipy import misc                # scipy.misc.imsave has been deprecated in newer Scipy versions.\n",
    "import imageio                        # try this if you got error on 'scipy.misc' has no attribute 'imread'\n",
    "\n",
    "import sys\n",
    "import random\n",
    "from time import sleep\n",
    "import subprocess                    # Popen in store_revision_info() will need it\n",
    "from skimage.transform import resize  # try this if you got error on 'scipy.misc' has no attribute 'imresize', scipy.misc.imresize is deprecated"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def layer(op):\n",
    "    \"\"\"Decorator for composable network layers.\"\"\"\n",
    "\n",
    "    def layer_decorated(self, *args, **kwargs):\n",
    "        # Automatically set a name if not provided.\n",
    "        name = kwargs.setdefault('name', self.get_unique_name(op.__name__)) # if name is not given, name = get_unique_name(op.__name__)\n",
    "        # Figure out the layer inputs.\n",
    "        if len(self.terminals) == 0:\n",
    "            raise RuntimeError('No input variables found for layer %s.' % name)\n",
    "        elif len(self.terminals) == 1:\n",
    "            layer_input = self.terminals[0]\n",
    "        else:\n",
    "            layer_input = list(self.terminals)\n",
    "        # Perform the operation and get the output.\n",
    "        layer_output = op(self, layer_input, *args, **kwargs)  # layer_output = layerfunction(self, layer_input, *args, **kwargs)\n",
    "        # Add to layer LUT.                                    # e.g. layer_output = prelu(self, inp)\n",
    "        self.layers[name] = layer_output\n",
    "        # This output is now the input for the next layer.\n",
    "        self.feed(layer_output)\n",
    "        # Return self for chained calls.\n",
    "        return self\n",
    "\n",
    "    return layer_decorated"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Network(object):\n",
    "\n",
    "    def __init__(self, inputs, trainable=True):   # constructor : define object in network class, and attribute(inputs, trainable)\n",
    "        # The input nodes for this network\n",
    "        self.inputs = inputs\n",
    "        # The current list of terminal nodes\n",
    "        self.terminals = []\n",
    "        # Mapping from layer names to layers\n",
    "        self.layers = dict(inputs)\n",
    "        # If true, the resulting variables are set as trainable\n",
    "        self.trainable = trainable\n",
    "\n",
    "        self.setup()\n",
    "\n",
    "    def setup(self):  # Method\n",
    "        \"\"\"Construct the network. \"\"\"\n",
    "        raise NotImplementedError('Must be implemented by the subclass.')\n",
    "\n",
    "    def load(self, data_path, session, ignore_missing=False):   # e.g. pnet.load(path, sess) in \"def create_mtcnn\" below\n",
    "        \"\"\"Load network weights.\n",
    "        \"\"\"\n",
    "        data_dict = np.load(data_path, encoding='latin1').item() #pylint: disable=no-member\n",
    "\n",
    "        for op_name in data_dict:\n",
    "            with tf.variable_scope(op_name, reuse=True):\n",
    "                for param_name, data in iteritems(data_dict[op_name]):\n",
    "                    try:\n",
    "                        var = tf.get_variable(param_name)\n",
    "                        session.run(var.assign(data))\n",
    "                    except ValueError:\n",
    "                        if not ignore_missing:\n",
    "                            raise\n",
    "\n",
    "    def feed(self, *args):\n",
    "        \"\"\"Set the input(s) for the next operation by replacing the terminal nodes.\n",
    "        The arguments can be either layer names or the actual layers.\n",
    "        \"\"\"\n",
    "        assert len(args) != 0\n",
    "        self.terminals = []\n",
    "        for fed_layer in args:\n",
    "            if isinstance(fed_layer, string_types):\n",
    "                try:\n",
    "                    fed_layer = self.layers[fed_layer]\n",
    "                except KeyError:\n",
    "                    raise KeyError('Unknown layer name fed: %s' % fed_layer)\n",
    "            self.terminals.append(fed_layer)\n",
    "        return self\n",
    "\n",
    "    def get_output(self):\n",
    "        \"\"\"Returns the current network output.\"\"\"\n",
    "        return self.terminals[-1]\n",
    "\n",
    "    def get_unique_name(self, prefix):\n",
    "        \"\"\"Returns an index-suffixed unique name for the given prefix.\n",
    "        This is used for auto-generating layer names based on the type-prefix.\n",
    "        \"\"\"\n",
    "        ident = sum(t.startswith(prefix) for t, _ in self.layers.items()) + 1\n",
    "        return '%s_%d' % (prefix, ident)     # e.g return conv3 for 3rd conv in all the layers\n",
    "\n",
    "    def make_var(self, name, shape):\n",
    "        \"\"\"Creates a new TensorFlow variable.\"\"\"\n",
    "        return tf.get_variable(name, shape, trainable=self.trainable)\n",
    "\n",
    "    def validate_padding(self, padding):\n",
    "        \"\"\"Verifies that the padding is one of the supported ones.\"\"\"\n",
    "        assert padding in ('SAME', 'VALID')\n",
    "\n",
    "    @layer\n",
    "    def conv(self,\n",
    "             inp,\n",
    "             k_h,     # kernel size_h-direct.\n",
    "             k_w,     # kernel size_w-direct.\n",
    "             c_o,     # filters (channel_outter)\n",
    "             s_h,     # stride_h-direct.\n",
    "             s_w,     # stride_w-direct.\n",
    "             name,                                            # define op_name here, then you can fine the weights in after\n",
    "             relu=True,\n",
    "             padding='SAME',                                  # notice here : the default \"SAME\" padding\n",
    "             group=1,\n",
    "             biased=True):\n",
    "        self.validate_padding(padding)                        # Verify that the padding is acceptable\n",
    "        c_i = int(inp.get_shape()[-1])                        # Get the number of channels in the input (channel_inner)\n",
    "        # Verify that the grouping parameter is valid\n",
    "        assert c_i % group == 0                              # % 1 == 0, assert value c_i is able to work\n",
    "        assert c_o % group == 0                              # % 1 == 0, assert value c_o is able to work\n",
    "        \n",
    "        # Convolution for a given input and kernel\n",
    "        convolve = lambda i, k: tf.nn.conv2d(input = i, filter = k, strides = [1, s_h, s_w, 1], padding=padding)  \n",
    "        with tf.variable_scope(name) as scope:\n",
    "            kernel = self.make_var('weights', shape=[k_h, k_w, c_i // group, c_o])       # define parameter_name here\n",
    "            # This is the common-case. Convolve the input without any further complications.\n",
    "            output = convolve(inp, kernel)                                               # define w*a\n",
    "            # Add the biases\n",
    "            if biased:\n",
    "                biases = self.make_var('biases', [c_o])                                  # define parameter_name here\n",
    "                output = tf.nn.bias_add(output, biases)                                  # define w*a + b\n",
    "            if relu:\n",
    "                # ReLU non-linearity\n",
    "                output = tf.nn.relu(output, name=scope.name)                             # define activation (w*a + b)\n",
    "            return output\n",
    "\n",
    "    @layer\n",
    "    def prelu(self, inp, name):\n",
    "        with tf.variable_scope(name):\n",
    "            i = int(inp.get_shape()[-1])\n",
    "            alpha = self.make_var('alpha', shape=(i,))                                   # define prelu parameter_name here\n",
    "            output = tf.nn.relu(inp) + tf.multiply(alpha, -tf.nn.relu(-inp))             # define prelu = ReLu(a) - alpha*ReLu(-a)\n",
    "        return output\n",
    "    # MTCNN conduct the nonlinearity by PReLU, 使用 Parametric ReLU (PReLU) 導入 nonlinearity activation\n",
    "    # alpha is a learnable variable 是需要學習的參數\n",
    "    \n",
    "    @layer\n",
    "    def max_pool(self, inp, k_h, k_w, s_h, s_w, name, padding='SAME'):                  # notice here : the default \"SAME\" padding\n",
    "        self.validate_padding(padding)\n",
    "        return tf.nn.max_pool(value = inp,\n",
    "                              ksize=[1, k_h, k_w, 1],\n",
    "                              strides=[1, s_h, s_w, 1],\n",
    "                              padding=padding,\n",
    "                              name=name)\n",
    "\n",
    "    @layer\n",
    "    def fc(self, inp, num_out, name, relu=True):\n",
    "        with tf.variable_scope(name):\n",
    "            input_shape = inp.get_shape()                        # get input shape, e.g. RNet fc input is [None, 3, 3, 64] [576, 128]\n",
    "            if input_shape.ndims == 4:\n",
    "                # The input is spatial. Vectorize it first.\n",
    "                dim = 1\n",
    "                for d in input_shape[1:].as_list():             # e.g. d = 3, 3, 64\n",
    "                    dim *= int(d)                               # e.g. dim = 3 * 3 * 64\n",
    "                feed_in = tf.reshape(inp, [-1, dim])            # e.g. feed_in = tf.reshape([None, 3, 3, 64], [576])\n",
    "            else:\n",
    "                feed_in, dim = (inp, input_shape[-1].value)     \n",
    "            weights = self.make_var('weights', shape=[dim, num_out])   # e.g. shape = [3 * 3 * 64, 128]\n",
    "            biases = self.make_var('biases', [num_out])                # e.g. shape = [128]\n",
    "            op = tf.nn.relu_layer if relu else tf.nn.xw_plus_b        # define operation = ReLu or w*a + b\n",
    "            fc = op(feed_in, weights, biases, name=name)\n",
    "            return fc\n",
    "\n",
    "    \"\"\"\n",
    "    Multi dimensional softmax,\n",
    "    refer to https://github.com/tensorflow/tensorflow/issues/210\n",
    "    compute softmax along the dimension of target\n",
    "    the native softmax only supports batch_size x dimension\n",
    "    \"\"\"\n",
    "    @layer\n",
    "    def softmax(self, target, axis, name=None):\n",
    "        max_axis = tf.reduce_max(target, axis, keepdims=True)\n",
    "        target_exp = tf.exp(target-max_axis)\n",
    "        normalize = tf.reduce_sum(target_exp, axis, keepdims=True)\n",
    "        softmax = tf.div(target_exp, normalize, name)\n",
    "        return softmax"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class PNet(Network):  # Decorate with Network\n",
    "    def setup(self):\n",
    "        (self.feed('data') #pylint: disable=no-value-for-parameter, no-member\n",
    "             .conv(3, 3, 10, 1, 1, padding='VALID', relu=False, name='conv1')\n",
    "             .prelu(name='PReLU1')\n",
    "             .max_pool(2, 2, 2, 2, name='pool1')\n",
    "             .conv(3, 3, 16, 1, 1, padding='VALID', relu=False, name='conv2')\n",
    "             .prelu(name='PReLU2')\n",
    "             .conv(3, 3, 32, 1, 1, padding='VALID', relu=False, name='conv3')\n",
    "             .prelu(name='PReLU3')\n",
    "             .conv(1, 1, 2, 1, 1, relu=False, name='conv4-1')\n",
    "             .softmax(3,name='prob1'))\n",
    "        (self.feed('PReLU3') #pylint: disable=no-value-for-parameter\n",
    "             .conv(1, 1, 4, 1, 1, relu=False, name='conv4-2'))\n",
    "        \n",
    "class RNet(Network):\n",
    "    def setup(self):\n",
    "        (self.feed('data') #pylint: disable=no-value-for-parameter, no-member\n",
    "             .conv(3, 3, 28, 1, 1, padding='VALID', relu=False, name='conv1')\n",
    "             .prelu(name='prelu1')\n",
    "             .max_pool(3, 3, 2, 2, name='pool1')    # notice here is default padding = \"SAME\", thus why result [11,11, 28]\n",
    "             .conv(3, 3, 48, 1, 1, padding='VALID', relu=False, name='conv2')\n",
    "             .prelu(name='prelu2')\n",
    "             .max_pool(3, 3, 2, 2, padding='VALID', name='pool2')\n",
    "             .conv(2, 2, 64, 1, 1, padding='VALID', relu=False, name='conv3')\n",
    "             .prelu(name='prelu3')\n",
    "             .fc(128, relu=False, name='conv4')\n",
    "             .prelu(name='prelu4')\n",
    "             .fc(2, relu=False, name='conv5-1')\n",
    "             .softmax(1,name='prob1'))\n",
    "\n",
    "        (self.feed('prelu4') #pylint: disable=no-value-for-parameter\n",
    "             .fc(4, relu=False, name='conv5-2'))\n",
    "        \n",
    "class ONet(Network):\n",
    "    def setup(self):\n",
    "        (self.feed('data') #pylint: disable=no-value-for-parameter, no-member\n",
    "             .conv(3, 3, 32, 1, 1, padding='VALID', relu=False, name='conv1')\n",
    "             .prelu(name='prelu1')\n",
    "             .max_pool(3, 3, 2, 2, name='pool1')\n",
    "             .conv(3, 3, 64, 1, 1, padding='VALID', relu=False, name='conv2')\n",
    "             .prelu(name='prelu2')\n",
    "             .max_pool(3, 3, 2, 2, padding='VALID', name='pool2')\n",
    "             .conv(3, 3, 64, 1, 1, padding='VALID', relu=False, name='conv3')\n",
    "             .prelu(name='prelu3')\n",
    "             .max_pool(2, 2, 2, 2, name='pool3')\n",
    "             .conv(2, 2, 128, 1, 1, padding='VALID', relu=False, name='conv4')\n",
    "             .prelu(name='prelu4')\n",
    "             .fc(256, relu=False, name='conv5')\n",
    "             .prelu(name='prelu5')\n",
    "             .fc(2, relu=False, name='conv6-1')\n",
    "             .softmax(1, name='prob1'))\n",
    "\n",
    "        (self.feed('prelu5') #pylint: disable=no-value-for-parameter\n",
    "             .fc(4, relu=False, name='conv6-2'))\n",
    "\n",
    "        (self.feed('prelu5') #pylint: disable=no-value-for-parameter\n",
    "             .fc(10, relu=False, name='conv6-3'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# function [boundingbox] = bbreg(boundingbox,reg)\n",
    "def bbreg(boundingbox,reg):\n",
    "    \"\"\"Calibrate bounding boxes\"\"\"\n",
    "    if reg.shape[1]==1:\n",
    "        reg = np.reshape(reg, (reg.shape[2], reg.shape[3]))\n",
    "\n",
    "    w = boundingbox[:,2]-boundingbox[:,0]+1\n",
    "    h = boundingbox[:,3]-boundingbox[:,1]+1\n",
    "    b1 = boundingbox[:,0]+reg[:,0]*w\n",
    "    b2 = boundingbox[:,1]+reg[:,1]*h\n",
    "    b3 = boundingbox[:,2]+reg[:,2]*w\n",
    "    b4 = boundingbox[:,3]+reg[:,3]*h\n",
    "    boundingbox[:,0:4] = np.transpose(np.vstack([b1, b2, b3, b4 ]))\n",
    "    return boundingbox\n",
    " \n",
    "def generateBoundingBox(imap, reg, scale, t):\n",
    "    \"\"\"Use heatmap to generate bounding boxes    \n",
    "    \"\"\"\n",
    "    stride=2                       # because max pooling stride = 2\n",
    "    cellsize=12\n",
    "\n",
    "    imap = np.transpose(imap)      # 輸出結果將為[num_w_slide, num_h_slide], 相當於pyramid中每張圖片sliding window後的結果\n",
    "    dx1 = np.transpose(reg[:,:,0]) # x1 for each bounding_box in imap, (shape = [num_w_slide, num_h_slide])\n",
    "    dy1 = np.transpose(reg[:,:,1]) # y1 for each bounding_box in imap, (shape = [num_w_slide, num_h_slide])\n",
    "    dx2 = np.transpose(reg[:,:,2]) # x2 for each bounding_box in imap, (shape = [num_w_slide, num_h_slide])\n",
    "    dy2 = np.transpose(reg[:,:,3]) # y2 for each bounding_box in imap, (shape = [num_w_slide, num_h_slide])\n",
    "    y, x = np.where(imap >= t)     # index mask for face_classification[num_w_slide, num_h_slide], see which one >= threshold[0]\n",
    "                                   # we can know which sliding window >= threshold[0], by coordinate (num_w_slide=y, num_h_slide=x)\n",
    "                                   # e.g. num_w_slide = y = [108 149 149 180], num_h_slide = x = [125 161 162 229]\n",
    "    \n",
    "    if y.shape[0]==1:             # if only 1 window was detected >= threshold[0]\n",
    "        dx1 = np.flipud(dx1)      # why flip over ?? when only 1 detected??\n",
    "        dy1 = np.flipud(dy1)      # why flip over ?? when only 1 detected??\n",
    "        dx2 = np.flipud(dx2)      # why flip over ?? when only 1 detected??\n",
    "        dy2 = np.flipud(dy2)      # why flip over ?? when only 1 detected??\n",
    "    score = imap[(y,x)]\n",
    "    reg = np.transpose(np.vstack([ dx1[(y,x)], dy1[(y,x)], dx2[(y,x)], dy2[(y,x)] ])) # vstack ensure matrix formate, final shape = [(num of > threshold), 4]\n",
    "    \n",
    "    if reg.size==0:               # if no bounding box was found\n",
    "        reg = np.empty((0,3))\n",
    "    bb = np.transpose(np.vstack([y,x]))          # bb = bounding box index\n",
    "    q1 = np.fix((stride*bb+1)/scale)             # q1 = turn window back to original pic by index\n",
    "    q2 = np.fix((stride*bb+cellsize-1+1)/scale) # q2 = turn window back to original pic by q1 + cellsize\n",
    "    boundingbox = np.hstack([q1, q2, np.expand_dims(score,1), reg])\n",
    "    return boundingbox, reg\n",
    " \n",
    "# function pick = nms(boxes,threshold,type)\n",
    "def nms(boxes, threshold, method):         # non-max suppression\n",
    "    if boxes.size==0:\n",
    "        return np.empty((0,3))\n",
    "    x1 = boxes[:,0] # box coordinate x1\n",
    "    y1 = boxes[:,1] # box coordinate y1\n",
    "    x2 = boxes[:,2] # box coordinate x2\n",
    "    y2 = boxes[:,3] # box coordinate y2\n",
    "    s = boxes[:,4]  # box score\n",
    "    area = (x2-x1+1) * (y2-y1+1)            # calculate box area\n",
    "    I = np.argsort(s)                       # return the index from min to max\n",
    "    pick = np.zeros_like(s, dtype=np.int16) # zeros matrix with same dimension in s.shape\n",
    "    counter = 0\n",
    "    while I.size>0:\n",
    "        i = I[-1]                           # reverse direction, which means from max to min.\n",
    "        pick[counter] = i                   # set pick[current_max_score_index, current_max_score_index ...] \n",
    "        counter += 1\n",
    "        idx = I[0:-1]                       # the rest of boxes, except of max score one\n",
    "        xx1 = np.maximum(x1[i], x1[idx])    # get intersection x1, np.maximum(current max ,rest boxes)\n",
    "        yy1 = np.maximum(y1[i], y1[idx])    # get intersection y1\n",
    "        xx2 = np.minimum(x2[i], x2[idx])    # get intersection x2\n",
    "        yy2 = np.minimum(y2[i], y2[idx])    # get intersection y2\n",
    "        w = np.maximum(0.0, xx2-xx1+1)      # intersection w\n",
    "        h = np.maximum(0.0, yy2-yy1+1)      # intersection h\n",
    "        inter = w * h                       # intersection area\n",
    "        if method is 'Min':\n",
    "            o = inter / np.minimum(area[i], area[idx])\n",
    "        else:\n",
    "            o = inter / (area[i] + area[idx] - inter)\n",
    "        I = I[np.where(o<=threshold)]       # under current max score, delet IoU > threshold\n",
    "    pick = pick[0:counter]\n",
    "    return pick\n",
    "\n",
    "# function [dy edy dx edx y ey x ex tmpw tmph] = pad(total_boxes,w,h)\n",
    "def pad(total_boxes, w, h):\n",
    "    \"\"\"Compute the padding coordinates (pad the bounding boxes to square)\"\"\"\n",
    "    tmpw = (total_boxes[:,2]-total_boxes[:,0]+1).astype(np.int32)\n",
    "    tmph = (total_boxes[:,3]-total_boxes[:,1]+1).astype(np.int32)\n",
    "    numbox = total_boxes.shape[0]\n",
    "\n",
    "    dx = np.ones((numbox), dtype=np.int32)\n",
    "    dy = np.ones((numbox), dtype=np.int32)\n",
    "    edx = tmpw.copy().astype(np.int32)\n",
    "    edy = tmph.copy().astype(np.int32)\n",
    "\n",
    "    x = total_boxes[:,0].copy().astype(np.int32)   # bb x1 coord. (w-dir.)\n",
    "    y = total_boxes[:,1].copy().astype(np.int32)   # bb y1 coord. (h-dir.)\n",
    "    ex = total_boxes[:,2].copy().astype(np.int32)  # bb x2 coord. (w-dir.)\n",
    "    ey = total_boxes[:,3].copy().astype(np.int32)  # bb y2 coord. (h-dir.)\n",
    "\n",
    "    tmp = np.where(ex>w) # index of those x2 coordinate > window\n",
    "    edx.flat[tmp] = np.expand_dims(-ex[tmp]+w+tmpw[tmp],1)\n",
    "    ex[tmp] = w\n",
    "    \n",
    "    tmp = np.where(ey>h) # index of those y2 coordinate > window\n",
    "    edy.flat[tmp] = np.expand_dims(-ey[tmp]+h+tmph[tmp],1)\n",
    "    ey[tmp] = h\n",
    "\n",
    "    tmp = np.where(x<1)  # index of those x1 coordinate < 1\n",
    "    dx.flat[tmp] = np.expand_dims(2-x[tmp],1)\n",
    "    x[tmp] = 1\n",
    "\n",
    "    tmp = np.where(y<1)  # index of those y1 coordinate < 1\n",
    "    dy.flat[tmp] = np.expand_dims(2-y[tmp],1)\n",
    "    y[tmp] = 1\n",
    "    # tmp[dy[k]-1:edy[k],dx[k]-1:edx[k],:] = img[y[k]-1:ey[k],x[k]-1:ex[k],:]\n",
    "    # bb coord. in target (dy,edy,dx,edx), bb coord. in source（y,ey,x,ex）\n",
    "    # set a suitable pixel corresponding to original img\n",
    "    return dy, edy, dx, edx, y, ey, x, ex, tmpw, tmph\n",
    "\n",
    "# function [bboxA] = rerec(bboxA)\n",
    "def rerec(bboxA):\n",
    "    \"\"\"Convert bboxA to square.\"\"\"\n",
    "    # re-rectangular box\n",
    "    h = bboxA[:,3]-bboxA[:,1]\n",
    "    w = bboxA[:,2]-bboxA[:,0]\n",
    "    l = np.maximum(w, h)\n",
    "    bboxA[:,0] = bboxA[:,0]+w*0.5-l*0.5 # try to expand the shorter edge to rectangular\n",
    "    bboxA[:,1] = bboxA[:,1]+h*0.5-l*0.5 # try to expand the shorter edge to rectangular\n",
    "    bboxA[:,2:4] = bboxA[:,0:2] + np.transpose(np.tile(l,(2,1)))\n",
    "    return bboxA\n",
    "\n",
    "def imresample(img, sz):\n",
    "    im_data = cv2.resize(img, (sz[1], sz[0]), interpolation=cv2.INTER_AREA) #@UndefinedVariable\n",
    "            # 5-way to interpolation in cv2 : INTER_NEAREST、INTER_LINEAR、INTER_AREA、INTER_CUBICI、NTER_LANCZOS4\n",
    "            # cv2.resize (h, w) 是相反的\n",
    "    return im_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_mtcnn(sess, model_path):\n",
    "    if not model_path:\n",
    "        model_path = os.getcwd()\n",
    "\n",
    "    np.load.__defaults__=(None, True, True, 'latin1')   # declare load.__defaults__\n",
    "\n",
    "    with tf.variable_scope('pnet'):\n",
    "        data = tf.placeholder(tf.float32, (None,None,None,3), 'input')\n",
    "        pnet = PNet({'data':data})  # define object pnet is PNet class\n",
    "        #pnet.load(os.path.join(model_path, 'det1.npy'), sess)\n",
    "        src_path = os.getcwd()\n",
    "        model_path = os.path.join(src_path, args.model)\n",
    "        pnet.load(os.path.join(model_path, 'det1.npy'), sess)\n",
    "        \n",
    "    with tf.variable_scope('rnet'):\n",
    "        data = tf.placeholder(tf.float32, (None,24,24,3), 'input')\n",
    "        rnet = RNet({'data':data})\n",
    "        src_path = os.getcwd()\n",
    "        model_path = os.path.join(src_path, args.model)\n",
    "        rnet.load(os.path.join(model_path, 'det2.npy'), sess)\n",
    "    with tf.variable_scope('onet'):\n",
    "        data = tf.placeholder(tf.float32, (None,48,48,3), 'input')\n",
    "        onet = ONet({'data':data})\n",
    "        src_path = os.getcwd()\n",
    "        model_path = os.path.join(src_path, args.model)\n",
    "        onet.load(os.path.join(model_path, 'det3.npy'), sess)\n",
    "            \n",
    "    np.load.__defaults__=(None, False, True, 'ASCII')  # load.__defaults__  go back to original default\n",
    "    \n",
    "    pnet_fun = lambda img : sess.run(('pnet/conv4-2/BiasAdd:0', 'pnet/prob1:0'), feed_dict={'pnet/input:0':img})\n",
    "    rnet_fun = lambda img : sess.run(('rnet/conv5-2/conv5-2:0', 'rnet/prob1:0'), feed_dict={'rnet/input:0':img})\n",
    "    onet_fun = lambda img : sess.run(('onet/conv6-2/conv6-2:0', 'onet/conv6-3/conv6-3:0', 'onet/prob1:0'), feed_dict={'onet/input:0':img})\n",
    "    return pnet_fun, rnet_fun, onet_fun"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def detect_face(img, minsize, pnet, rnet, onet, threshold, factor):\n",
    "    \"\"\"Detects faces in an image, and returns bounding boxes and points for them.\n",
    "    img: input image\n",
    "    minsize: minimum faces' size\n",
    "    pnet, rnet, onet: caffemodel\n",
    "    threshold: threshold=[th1, th2, th3], th1-3 are three steps's threshold\n",
    "    factor: the factor used to create a scaling pyramid of face sizes to detect in the image.\n",
    "    \"\"\"\n",
    "    factor_count=0\n",
    "    total_boxes=np.empty((0,9))\n",
    "    points=np.empty(0)\n",
    "    h=img.shape[0]        # e.g. h = 1500\n",
    "    w=img.shape[1]        # e.g. w = 989\n",
    "    minl=np.amin([h, w])  # minl = np.amin([h, w]) = 120 (get min. edge length)\n",
    "    m=12.0/minsize        # m = 12/20, 因為PNet是用12x12訓練, 所以如果要求的minsizee!=12, 則需縮放到12x才可使用PNet參數\n",
    "    minl=minl*m           # minl = 120 * (12/20)\n",
    "\n",
    "    # create scale list for pyramid 建立金字塔所需scale list\n",
    "    scales=[]\n",
    "    while minl>=12:             # while PNet inpout > 12, e.g. minl = 120*(12/20), 120*(12/20)*f, 120*(12/20)*f^2, ...\n",
    "        scales += [m*np.power(factor, factor_count)]  # scale += m * (f^(0,1,2,...)), till minl < 12, e.g. 12/20 *f^0, 12/20 *(f^0 + f^1),\n",
    "        minl = minl*factor                            # minl = minl * f, e.g. 120 * (12/20) * f, 120 * (12/20) * f^2, \n",
    "        factor_count += 1                             # factor_count = 0, 1, 2, 3... till minl < 12\n",
    "    \n",
    "    # first stage                                      # P-Net processing\n",
    "    for scale in scales:                              # e.g. 12/20 *f^0, 12/20 *(f^0 + f^1), ...\n",
    "        hs=int(np.ceil(h*scale))                      # sample h 依scale list逐次縮小到minsize\n",
    "        ws=int(np.ceil(w*scale))                      # sample w 依scale list逐次縮小到minsize\n",
    "        im_data = imresample(img, (hs, ws))           # im_data = 依scale list 逐次resample縮放到目標sample尺寸\n",
    "        im_data = (im_data-127.5)*0.0078125           # mean-zero & normalize\n",
    "        img_x = np.expand_dims(im_data, 0)            # expand dimension axis=0, [h_x, w_x, c_x] -> [None, h_x, w_x, c_x]\n",
    "        img_y = np.transpose(img_x, (0,2,1,3))        # transpose, [None, h_x, w_x, c_x] -> [None, w_x, h_x, c_x]\n",
    "        out = pnet(img_y)                             # PNet_out = [bounding_box, face_classification]\n",
    "        out0 = np.transpose(out[0], (0,2,1,3))        # bounding_box result, 輸出結果為[1, n_h_slide, n_w_slide, 4], n 即input image尺寸以12x12 作sliding window的數量\n",
    "        out1 = np.transpose(out[1], (0,2,1,3))        # face_classification result, 輸出結果為[1, n_h_slide, n_w_slide, 2], n 即input image尺寸以12x12 作sliding window的數量\n",
    "                                                      # [0] for probability of no face, [1]for probability of get face, [0] + [1] = 1\n",
    "        boxes, _ = generateBoundingBox(out1[0,:,:,1].copy(), out0[0,:,:,:].copy(), scale, threshold[0])\n",
    "                                                      # boxes = [None, 9]\n",
    "        # inter-scale nms\n",
    "        pick = nms(boxes.copy(), 0.5, 'Union')        # nms(boxes,threshold = 0.5,type = \"Union\")\n",
    "        if boxes.size>0 and pick.size>0:\n",
    "            boxes = boxes[pick,:]\n",
    "            total_boxes = np.append(total_boxes, boxes, axis=0)\n",
    "\n",
    "    numbox = total_boxes.shape[0]\n",
    "    if numbox>0:\n",
    "        pick = nms(total_boxes.copy(), 0.7, 'Union')   # nms(boxes,threshold = 0.7,type = \"Union\")\n",
    "        total_boxes = total_boxes[pick,:]              # shape = [None, 9]\n",
    "        regw = total_boxes[:,2]-total_boxes[:,0]       # sliding_window_width = q2(y) - q1(y)\n",
    "        regh = total_boxes[:,3]-total_boxes[:,1]       # sliding_window_height = q2(x) - q1(x)\n",
    "        qq1 = total_boxes[:,0]+total_boxes[:,5]*regw   # sliding window pixel_num + x1*width\n",
    "        qq2 = total_boxes[:,1]+total_boxes[:,6]*regh   # sliding window pixel_num + y1*height\n",
    "        qq3 = total_boxes[:,2]+total_boxes[:,7]*regw   # sliding window pixel_num + x2*width\n",
    "        qq4 = total_boxes[:,3]+total_boxes[:,8]*regh   # sliding window pixel_num + y2*height\n",
    "        total_boxes = np.transpose(np.vstack([qq1, qq2, qq3, qq4, total_boxes[:,4]])) # [qq1, qq2, qq3, qq4, score]\n",
    "        total_boxes = rerec(total_boxes.copy())        # re-rectangular box\n",
    "        total_boxes[:,0:4] = np.fix(total_boxes[:,0:4]).astype(np.int32)\n",
    "        dy, edy, dx, edx, y, ey, x, ex, tmpw, tmph = pad(total_boxes.copy(), w, h)\n",
    "                                                     # pad the bounding boxes to square), then we can send the box into RNet\n",
    "\n",
    "    numbox = total_boxes.shape[0]\n",
    "    if numbox>0:\n",
    "        # second stage\n",
    "        tempimg = np.zeros((24,24,3,numbox))\n",
    "        for k in range(0,numbox):\n",
    "            tmp = np.zeros((int(tmph[k]),int(tmpw[k]),3))\n",
    "            tmp[dy[k]-1:edy[k],dx[k]-1:edx[k],:] = img[y[k]-1:ey[k],x[k]-1:ex[k],:] # set a suitable pixel corresponding to original img\n",
    "            if tmp.shape[0]>0 and tmp.shape[1]>0 or tmp.shape[0]==0 and tmp.shape[1]==0:\n",
    "                tempimg[:,:,:,k] = imresample(tmp, (24, 24))  # resample into 24x24x3 before into RNet\n",
    "            else:\n",
    "                return np.empty()\n",
    "        tempimg = (tempimg-127.5)*0.0078125            # mean-zero & normalize\n",
    "        tempimg1 = np.transpose(tempimg, (3,1,0,2))\n",
    "        out = rnet(tempimg1)                           # RNet_out = [bounding_box result, face_classification result]\n",
    "        out0 = np.transpose(out[0])                    # bounding_box result, 輸出結果為[4, None]\n",
    "        out1 = np.transpose(out[1])                    # face_classification result, 輸出結果為[2, None]\n",
    "                                                       # [0] for probability of no face, [1]for probability of get face, [0] + [1] = 1\n",
    "        score = out1[1,:]\n",
    "        ipass = np.where(score>threshold[1])\n",
    "        total_boxes = np.hstack([total_boxes[ipass[0],0:4].copy(), np.expand_dims(score[ipass].copy(),1)])\n",
    "        mv = out0[:,ipass[0]]\n",
    "        if total_boxes.shape[0]>0:\n",
    "            pick = nms(total_boxes, 0.7, 'Union')      # nms(boxes,threshold = 0.7,type = \"Union\")\n",
    "            total_boxes = total_boxes[pick,:]\n",
    "            total_boxes = bbreg(total_boxes.copy(), np.transpose(mv[:,pick]))\n",
    "            total_boxes = rerec(total_boxes.copy())\n",
    "\n",
    "    numbox = total_boxes.shape[0]\n",
    "    if numbox>0:\n",
    "        # third stage\n",
    "        total_boxes = np.fix(total_boxes).astype(np.int32)\n",
    "        dy, edy, dx, edx, y, ey, x, ex, tmpw, tmph = pad(total_boxes.copy(), w, h)\n",
    "        tempimg = np.zeros((48,48,3,numbox))\n",
    "        for k in range(0,numbox):\n",
    "            tmp = np.zeros((int(tmph[k]),int(tmpw[k]),3))\n",
    "            tmp[dy[k]-1:edy[k],dx[k]-1:edx[k],:] = img[y[k]-1:ey[k],x[k]-1:ex[k],:]\n",
    "            if tmp.shape[0]>0 and tmp.shape[1]>0 or tmp.shape[0]==0 and tmp.shape[1]==0:\n",
    "                tempimg[:,:,:,k] = imresample(tmp, (48, 48))\n",
    "            else:\n",
    "                return np.empty()\n",
    "        tempimg = (tempimg-127.5)*0.0078125\n",
    "        tempimg1 = np.transpose(tempimg, (3,1,0,2))\n",
    "        out = onet(tempimg1)                           # ONet_out = [bounding_box result, facial landmark, face_classification result]\n",
    "        out0 = np.transpose(out[0])                    # bounding_box result, 輸出結果為[4, None]\n",
    "        out1 = np.transpose(out[1])                    # bounding_box result, 輸出結果為[10, None]\n",
    "        out2 = np.transpose(out[2])                    # bounding_box result, 輸出結果為[2, None]\n",
    "        score = out2[1,:]\n",
    "        points = out1\n",
    "        ipass = np.where(score>threshold[2])           \n",
    "        points = points[:,ipass[0]]\n",
    "        total_boxes = np.hstack([total_boxes[ipass[0],0:4].copy(), np.expand_dims(score[ipass].copy(),1)])\n",
    "        mv = out0[:,ipass[0]]\n",
    "\n",
    "        w = total_boxes[:,2]-total_boxes[:,0]+1\n",
    "        h = total_boxes[:,3]-total_boxes[:,1]+1\n",
    "        points[0:5,:] = np.tile(w,(5, 1))*points[0:5,:] + np.tile(total_boxes[:,0],(5, 1))-1\n",
    "        points[5:10,:] = np.tile(h,(5, 1))*points[5:10,:] + np.tile(total_boxes[:,1],(5, 1))-1\n",
    "        if total_boxes.shape[0]>0:\n",
    "            total_boxes = bbreg(total_boxes.copy(), np.transpose(mv))\n",
    "            pick = nms(total_boxes.copy(), 0.7, 'Min') # nms(boxes,threshold = 0.7,type = \"Min\")\n",
    "            total_boxes = total_boxes[pick,:]\n",
    "            points = points[:,pick]\n",
    "                \n",
    "    return total_boxes, points\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"Functions for building the face recognition network.\n",
    "\n",
    "   facenet.py\n",
    "   \n",
    "\"\"\"\n",
    "# MIT License\n",
    "# \n",
    "# Copyright (c) 2016 David Sandberg\n",
    "# \n",
    "# Permission is hereby granted, free of charge, to any person obtaining a copy\n",
    "# of this software and associated documentation files (the \"Software\"), to deal\n",
    "# in the Software without restriction, including without limitation the rights\n",
    "# to use, copy, modify, merge, publish, distribute, sublicense, and/or sell\n",
    "# copies of the Software, and to permit persons to whom the Software is\n",
    "# furnished to do so, subject to the following conditions:\n",
    "# \n",
    "# The above copyright notice and this permission notice shall be included in all\n",
    "# copies or substantial portions of the Software.\n",
    "# \n",
    "# THE SOFTWARE IS PROVIDED \"AS IS\", WITHOUT WARRANTY OF ANY KIND, EXPRESS OR\n",
    "# IMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY,\n",
    "# FITNESS FOR A PARTICULAR PURPOSE AND NONINFRINGEMENT. IN NO EVENT SHALL THE\n",
    "# AUTHORS OR COPYRIGHT HOLDERS BE LIABLE FOR ANY CLAIM, DAMAGES OR OTHER\n",
    "# LIABILITY, WHETHER IN AN ACTION OF CONTRACT, TORT OR OTHERWISE, ARISING FROM,\n",
    "# OUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR OTHER DEALINGS IN THE\n",
    "# SOFTWARE.\n",
    "\n",
    "# pylint: disable=missing-docstring\n",
    "from subprocess import Popen, PIPE\n",
    "from sklearn.model_selection import KFold\n",
    "from scipy import interpolate\n",
    "from tensorflow.python.training import training\n",
    "import re\n",
    "from tensorflow.python.platform import gfile\n",
    "import math\n",
    "from six import iteritems\n",
    "\n",
    "\n",
    "def load_model(model, input_map=None):\n",
    "    # Check if the model is a model directory (containing a metagraph and a checkpoint file)\n",
    "    #  or if it is a protobuf file with a frozen graph\n",
    "    src_path = os.getcwd()\n",
    "    model_path = os.path.join(src_path, args.model) # download pre-trained facenet ResNet model, put under the \"models\" folder\n",
    "    model_exp = os.path.expanduser(model_path)\n",
    "    if (os.path.isfile(model_exp)):\n",
    "        print('Model filename: %s' % model_exp)\n",
    "        with gfile.FastGFile(model_exp,'rb') as f:\n",
    "            graph_def = tf.GraphDef()\n",
    "            graph_def.ParseFromString(f.read())\n",
    "            tf.import_graph_def(graph_def, input_map=input_map, name='')\n",
    "    else:\n",
    "        print('Model directory: %s' % model_exp)\n",
    "        meta_file, ckpt_file = get_model_filenames(model_exp)\n",
    "        \n",
    "        print('Metagraph file: %s' % meta_file)\n",
    "        print('Checkpoint file: %s' % ckpt_file)\n",
    "      \n",
    "        saver = tf.train.import_meta_graph(os.path.join(model_exp, meta_file), input_map=input_map)\n",
    "        saver.restore(tf.get_default_session(), os.path.join(model_exp, ckpt_file))\n",
    "\n",
    "def get_model_filenames(model_dir):\n",
    "    files = os.listdir(model_dir)\n",
    "    meta_files = [s for s in files if s.endswith('.meta')]\n",
    "    if len(meta_files)==0:\n",
    "        raise ValueError('No meta file found in the model directory (%s)' % model_dir)\n",
    "    elif len(meta_files)>1:\n",
    "        raise ValueError('There should not be more than one meta file in the model directory (%s)' % model_dir)\n",
    "    meta_file = meta_files[0]\n",
    "    ckpt = tf.train.get_checkpoint_state(model_dir)\n",
    "    if ckpt and ckpt.model_checkpoint_path:\n",
    "        ckpt_file = os.path.basename(ckpt.model_checkpoint_path)\n",
    "        return meta_file, ckpt_file\n",
    "\n",
    "    meta_files = [s for s in files if '.ckpt' in s]\n",
    "    max_step = -1\n",
    "    for f in files:\n",
    "        step_str = re.match(r'(^model-[\\w\\- ]+.ckpt-(\\d+))', f)\n",
    "        if step_str is not None and len(step_str.groups())>=2:\n",
    "            step = int(step_str.groups()[1])\n",
    "            if step > max_step:\n",
    "                max_step = step\n",
    "                ckpt_file = step_str.groups()[0]\n",
    "    return meta_file, ckpt_file\n",
    "        \n",
    "def prewhiten(x):\n",
    "    mean = np.mean(x)\n",
    "    std = np.std(x)\n",
    "    std_adj = np.maximum(std, 1.0/np.sqrt(x.size))\n",
    "    y = np.multiply(np.subtract(x, mean), 1/std_adj)\n",
    "    return y  \n",
    "\n",
    "\n",
    "def get_dataset(path, has_class_directories=True):\n",
    "    dataset = []\n",
    "    path_exp = os.path.expanduser(path)\n",
    "    classes = [path for path in os.listdir(path_exp) \\\n",
    "                    if os.path.isdir(os.path.join(path_exp, path))]\n",
    "    classes.sort()\n",
    "    nrof_classes = len(classes)\n",
    "    for i in range(nrof_classes):\n",
    "        class_name = classes[i]\n",
    "        facedir = os.path.join(path_exp, class_name)\n",
    "        image_paths = get_image_paths(facedir)\n",
    "        dataset.append(ImageClass(class_name, image_paths))\n",
    "  \n",
    "    return dataset\n",
    "\n",
    "def get_image_paths(facedir):\n",
    "    image_paths = []\n",
    "    if os.path.isdir(facedir):\n",
    "        images = os.listdir(facedir)\n",
    "        image_paths = [os.path.join(facedir,img) for img in images]\n",
    "    return image_paths\n",
    "\n",
    "def get_image_paths_and_labels(dataset):\n",
    "    image_paths_flat = []\n",
    "    labels_flat = []\n",
    "    for i in range(len(dataset)):\n",
    "        image_paths_flat += dataset[i].image_paths\n",
    "        labels_flat += [i] * len(dataset[i].image_paths)\n",
    "    return image_paths_flat, labels_flat\n",
    "      \n",
    "        \n",
    "def load_data(image_paths, do_random_crop, do_random_flip, image_size, do_prewhiten=True):\n",
    "    nrof_samples = len(image_paths)\n",
    "    images = np.zeros((nrof_samples, image_size, image_size, 3))\n",
    "    for i in range(nrof_samples):\n",
    "        #img = misc.imread(image_paths[i])\n",
    "        img = imageio.imread(image_paths[i])         # try this if you got error on 'scipy.misc' has no attribute 'imread'\n",
    "        if img.ndim == 2:\n",
    "            img = to_rgb(img)\n",
    "        if do_prewhiten:\n",
    "            img = prewhiten(img)\n",
    "        img = crop(img, do_random_crop, image_size)\n",
    "        img = flip(img, do_random_flip)\n",
    "        images[i,:,:,:] = img\n",
    "    return images\n",
    "\n",
    "def crop(image, random_crop, image_size):\n",
    "    if image.shape[1]>image_size:\n",
    "        sz1 = int(image.shape[1]//2)\n",
    "        sz2 = int(image_size//2)\n",
    "        if random_crop:\n",
    "            diff = sz1-sz2\n",
    "            (h, v) = (np.random.randint(-diff, diff+1), np.random.randint(-diff, diff+1))\n",
    "        else:\n",
    "            (h, v) = (0,0)\n",
    "        image = image[(sz1-sz2+v):(sz1+sz2+v),(sz1-sz2+h):(sz1+sz2+h),:]\n",
    "    return image\n",
    "\n",
    "def flip(image, random_flip):\n",
    "    if random_flip and np.random.choice([True, False]):\n",
    "        image = np.fliplr(image)\n",
    "    return image\n",
    "\n",
    "class ImageClass():\n",
    "    \"Stores the paths to images for a given class\"\n",
    "    def __init__(self, name, image_paths):\n",
    "        self.name = name\n",
    "        self.image_paths = image_paths\n",
    "  \n",
    "    def __str__(self):\n",
    "        return self.name + ', ' + str(len(self.image_paths)) + ' images'\n",
    "  \n",
    "    def __len__(self):\n",
    "        return len(self.image_paths)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "import argparse                       # creat a parser to 從命令列直接讀取引數, 在jupyter運行時會因為sys.argv[1:]不為空而報錯\n",
    "\n",
    "parser = argparse.ArgumentParser()\n",
    "\n",
    "parser.add_argument('--mode', type=str, #choices=['TRAIN', 'CLASSIFY'],\n",
    "    help='Indicates if a new classifier should be trained or a classification ' + \n",
    "    'model should be used for classification', default='TRAIN')  # default could be either 'TRAIN' or 'CLASSIFY' , choices=['TRAIN', 'CLASSIFY']\n",
    "parser.add_argument('--data_dir', type=str, default=r\"input_dir\",\n",
    "    help='Path to the data directory containing aligned LFW face patches.')\n",
    "parser.add_argument('--model', type=str, default=r\"models\",\n",
    "    help='Could be either a directory containing the meta_file and ckpt_file or a model protobuf (.pb) file')\n",
    "parser.add_argument('--classifier_filename', default=\"my_classifier.pkl\",\n",
    "    help='Classifier model file name as a pickle (.pkl) file. ' + \n",
    "    'For training this is the output and for classification this is an input.')\n",
    "parser.add_argument('--use_split_dataset', \n",
    "    help='Indicates that the dataset specified by data_dir should be split into a training and test set. ' +  \n",
    "    'Otherwise a separate test set can be specified using the test_data_dir option.', action='store_true')\n",
    "parser.add_argument('--test_data_dir', type=str,  default=r\"input_dir\",\n",
    "    help='Path to the test data directory containing aligned images used for testing.')\n",
    "parser.add_argument('--batch_size', type=int,\n",
    "    help='Number of images to process in a batch.', default=90)\n",
    "parser.add_argument('--image_size', type=int,\n",
    "    help='Image size (height, width) in pixels.', default=160)\n",
    "parser.add_argument('--seed', type=int,\n",
    "    help='Random seed.', default=666)\n",
    "parser.add_argument('--min_nrof_images_per_class', type=int,\n",
    "    help='Only include classes with at least this number of images in the dataset', default=10)\n",
    "parser.add_argument('--nrof_train_images_per_class', type=int,\n",
    "    help='Use this number of images from each class for training and the rest for testing', default=5)\n",
    "parser.add_argument('--debug', action='store_true',\n",
    "                    help='Enable some debug outputs.')\n",
    "\n",
    "args = parser.parse_args(args=[\"--mode\", \"CLASSIFY\", \"--data_dir\", \"input_dir\", \"--model\", \"models\", \"--classifier_filename\", \"my_classifier.pkl\",\n",
    "                               \"--use_split_dataset\", \"--test_data_dir\", r\"input_dir\", \"--batch_size\", \"90\", \"--image_size\", \"160\",\n",
    "                               \"--seed\", \"666\", \"--min_nrof_images_per_class\", \"10\", \"--nrof_train_images_per_class\", \"5\",\n",
    "                               \"--debug\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preliminary work before you run 前置作業\n",
    "<br>Creat \"models\" folder, and put all medel parametes inside 創建 \"models\" 資料夾, 將所有參數放在此資料夾內\n",
    "<br><p style=\"text-align:left;\"><img src=\"images/dir_struc.png\"  style=\"width:861px;height:496px;\" align=\"left\">"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# coding=utf-8\n",
    "\"\"\"Performs face detection in realtime. Based on code from https://github.com/shanren7/real_time_face_recognition\"\"\"\n",
    "# MIT License\n",
    "#\n",
    "# Copyright (c) 2017 François Gervais\n",
    "#\n",
    "# This is the work of David Sandberg and shanren7 remodelled into a\n",
    "# high level container. It's an attempt to simplify the use of such\n",
    "# technology and provide an easy to use facial recognition package.\n",
    "#\n",
    "# https://github.com/davidsandberg/facenet\n",
    "# https://github.com/shanren7/real_time_face_recognition\n",
    "#\n",
    "# Permission is hereby granted, free of charge, to any person obtaining a copy\n",
    "# of this software and associated documentation files (the \"Software\"), to deal\n",
    "# in the Software without restriction, including without limitation the rights\n",
    "# to use, copy, modify, merge, publish, distribute, sublicense, and/or sell\n",
    "# copies of the Software, and to permit persons to whom the Software is\n",
    "# furnished to do so, subject to the following conditions:\n",
    "#\n",
    "# The above copyright notice and this permission notice shall be included in all\n",
    "# copies or substantial portions of the Software.\n",
    "#\n",
    "# THE SOFTWARE IS PROVIDED \"AS IS\", WITHOUT WARRANTY OF ANY KIND, EXPRESS OR\n",
    "# IMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY,\n",
    "# FITNESS FOR A PARTICULAR PURPOSE AND NONINFRINGEMENT. IN NO EVENT SHALL THE\n",
    "# AUTHORS OR COPYRIGHT HOLDERS BE LIABLE FOR ANY CLAIM, DAMAGES OR OTHER\n",
    "# LIABILITY, WHETHER IN AN ACTION OF CONTRACT, TORT OR OTHERWISE, ARISING FROM,\n",
    "# OUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR OTHER DEALINGS IN THE\n",
    "# SOFTWARE.\n",
    "\n",
    "import pickle\n",
    "import cv2\n",
    "import time\n",
    "#import align.detect_face\n",
    "#import facenet\n",
    "\n",
    "\n",
    "#gpu_memory_fraction = 0.3\n",
    "\n",
    "src_path = os.getcwd()\n",
    "model_path = os.path.join(src_path, args.model)\n",
    "facenet_model_checkpoint = model_path\n",
    "\n",
    "classifier_model_path = os.path.join(src_path, args.model)\n",
    "classifier_model = classifier_model_path = os.path.join(classifier_model_path, args.classifier_filename)\n",
    "\n",
    "\n",
    "debug = False\n",
    "\n",
    "\n",
    "class Face:\n",
    "    def __init__(self):\n",
    "        self.name = None\n",
    "        self.bounding_box = None\n",
    "        self.image = None\n",
    "        self.container_image = None\n",
    "        self.embedding = None\n",
    "        \n",
    "class Recognition:\n",
    "    def __init__(self):\n",
    "        self.detect = Detection()\n",
    "        self.encoder = Encoder()\n",
    "        self.identifier = Identifier()\n",
    "\n",
    "    def add_identity(self, image, person_name):\n",
    "        faces = self.detect.find_faces(image)\n",
    "\n",
    "        if len(faces) == 1:\n",
    "            face = faces[0]\n",
    "            face.name = person_name\n",
    "            face.embedding = self.encoder.generate_embedding(face)\n",
    "            return faces\n",
    "\n",
    "    def identify(self, image):\n",
    "        faces = self.detect.find_faces(image)\n",
    "\n",
    "        for i, face in enumerate(faces):\n",
    "            if debug:\n",
    "                cv2.imshow(\"Face: \" + str(i), face.image)\n",
    "            face.embedding = self.encoder.generate_embedding(face)\n",
    "            face.name = self.identifier.identify(face)\n",
    "\n",
    "        return faces\n",
    "\n",
    "\n",
    "class Identifier:\n",
    "    def __init__(self):\n",
    "        with open(classifier_model_path, 'rb') as infile:\n",
    "            self.model, self.class_names = pickle.load(infile)\n",
    "\n",
    "    def identify(self, face):\n",
    "        if face.embedding is not None:\n",
    "            predictions = self.model.predict_proba([face.embedding])\n",
    "            best_class_indices = np.argmax(predictions, axis=1)\n",
    "            return self.class_names[best_class_indices[0]]\n",
    "\n",
    "\n",
    "class Encoder:\n",
    "    def __init__(self):\n",
    "        self.sess = tf.Session()\n",
    "        with self.sess.as_default():\n",
    "            load_model(facenet_model_checkpoint)\n",
    "\n",
    "    def generate_embedding(self, face):\n",
    "        # Get input and output tensors\n",
    "        images_placeholder = tf.get_default_graph().get_tensor_by_name(\"input:0\")\n",
    "        embeddings = tf.get_default_graph().get_tensor_by_name(\"embeddings:0\")\n",
    "        phase_train_placeholder = tf.get_default_graph().get_tensor_by_name(\"phase_train:0\")\n",
    "\n",
    "        #prewhiten_face = facenet.prewhiten(face.image)\n",
    "        prewhiten_face = prewhiten(face.image)\n",
    "\n",
    "        # Run forward pass to calculate embeddings\n",
    "        feed_dict = {images_placeholder: [prewhiten_face], phase_train_placeholder: False}\n",
    "        return self.sess.run(embeddings, feed_dict=feed_dict)[0]\n",
    "\n",
    "\n",
    "class Detection:\n",
    "    # face detection parameters\n",
    "    minsize = 20  # minimum size of face\n",
    "    threshold = [0.6, 0.7, 0.7]  # three steps's threshold\n",
    "    factor = 0.709  # scale factor\n",
    "\n",
    "    def __init__(self, face_crop_size=160, face_crop_margin=32):\n",
    "        self.pnet, self.rnet, self.onet = self._setup_mtcnn()\n",
    "        self.face_crop_size = face_crop_size\n",
    "        self.face_crop_margin = face_crop_margin\n",
    "\n",
    "    def _setup_mtcnn(self):\n",
    "        with tf.Graph().as_default():\n",
    "            #gpu_options = tf.GPUOptions(per_process_gpu_memory_fraction=gpu_memory_fraction)\n",
    "            #sess = tf.Session(config=tf.ConfigProto(gpu_options=gpu_options, log_device_placement=False))\n",
    "            sess = tf.Session(config=tf.ConfigProto(log_device_placement=False))\n",
    "            with sess.as_default():\n",
    "                #return align.detect_face.create_mtcnn(sess, None)\n",
    "                return create_mtcnn(sess, None)\n",
    "\n",
    "    def find_faces(self, image):\n",
    "        faces = []\n",
    "\n",
    "        bounding_boxes, _ = detect_face(image, self.minsize,\n",
    "                                        self.pnet, self.rnet, self.onet,\n",
    "                                        self.threshold, self.factor)\n",
    "        for bb in bounding_boxes:\n",
    "            face = Face()\n",
    "            face.container_image = image\n",
    "            face.bounding_box = np.zeros(4, dtype=np.int32)\n",
    "\n",
    "            img_size = np.asarray(image.shape)[0:2]\n",
    "            face.bounding_box[0] = np.maximum(bb[0] - self.face_crop_margin / 2, 0)\n",
    "            face.bounding_box[1] = np.maximum(bb[1] - self.face_crop_margin / 2, 0)\n",
    "            face.bounding_box[2] = np.minimum(bb[2] + self.face_crop_margin / 2, img_size[1])\n",
    "            face.bounding_box[3] = np.minimum(bb[3] + self.face_crop_margin / 2, img_size[0])\n",
    "            cropped = image[face.bounding_box[1]:face.bounding_box[3], face.bounding_box[0]:face.bounding_box[2], :]\n",
    "            #face.image = misc.imresize(cropped, (self.face_crop_size, self.face_crop_size), interp='bilinear')\n",
    "            face.image = resize(cropped, output_shape=(self.face_crop_size,self.face_crop_size))  # try this if you got error on 'scipy.misc' has no attribute 'imresize', scipy.misc.imre\n",
    "\n",
    "            faces.append(face)\n",
    "\n",
    "        return faces\n",
    "    \n",
    "def add_overlays(frame, faces, frame_rate):\n",
    "    if faces is not None:\n",
    "        for face in faces:\n",
    "            face_bb = face.bounding_box.astype(int)\n",
    "            cv2.rectangle(frame,\n",
    "                          (face_bb[0], face_bb[1]), (face_bb[2], face_bb[3]),\n",
    "                          (255, 0, 0), 2)\n",
    "            if face.name is not None:\n",
    "                cv2.putText(frame, face.name, (face_bb[0], face_bb[3]),\n",
    "                            cv2.FONT_HERSHEY_SIMPLEX, 1, (0, 255, 0),\n",
    "                            thickness=2, lineType=2)\n",
    "\n",
    "    cv2.putText(frame, str(frame_rate) + \" fps\", (10, 30),\n",
    "                cv2.FONT_HERSHEY_SIMPLEX, 1, (0, 255, 0),\n",
    "                thickness=2, lineType=2)\n",
    "\n",
    "\n",
    "def main(args):\n",
    "    frame_interval = 3  # Number of frames after which to run face detection\n",
    "    fps_display_interval = 5  # seconds\n",
    "    frame_rate = 0\n",
    "    frame_count = 0\n",
    "\n",
    "    video_capture = cv2.VideoCapture(0)\n",
    "    #face_recognition = face.Recognition()\n",
    "    face_recognition = Recognition()\n",
    "    start_time = time.time()\n",
    "\n",
    "    if args.debug:\n",
    "        print(\"Debug enabled\")\n",
    "        #face.debug = True\n",
    "        debug = True\n",
    "\n",
    "    while(video_capture.isOpened()) :        # check ! (better usage than \"while true\")\n",
    "        ret, frame = video_capture.read()    # capture frame-by-frame\n",
    "        \n",
    "        if (frame_count % frame_interval) == 0:\n",
    "            faces = face_recognition.identify(frame)\n",
    "\n",
    "            # Check our current fps\n",
    "            end_time = time.time()\n",
    "            if (end_time - start_time) > fps_display_interval:\n",
    "                frame_rate = int(frame_count / (end_time - start_time))\n",
    "                start_time = time.time()\n",
    "                frame_count = 0\n",
    "\n",
    "        add_overlays(frame, faces, frame_rate)\n",
    "\n",
    "        frame_count += 1\n",
    "        cv2.imshow('Video', frame)\n",
    "\n",
    "        if cv2.waitKey(1) & 0xFF == ord('q'):\n",
    "            break            \n",
    "            \n",
    "    # When everything is done, release the capture\n",
    "    video_capture.release()\n",
    "    cv2.destroyAllWindows()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From C:\\Users\\user\\anaconda3\\envs\\py37_tf\\lib\\site-packages\\tensorflow\\python\\ops\\init_ops.py:1251: calling VarianceScaling.__init__ (from tensorflow.python.ops.init_ops) with dtype is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Call initializer instance with the dtype argument instead of passing it to the constructor\n",
      "WARNING:tensorflow:From <ipython-input-3-80902b31c8c5>:150: div (from tensorflow.python.ops.math_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Deprecated in favor of operator or tf.math.divide.\n",
      "Model directory: C:\\Users\\user\\face_recog\\DGEs\\FaceNet\\step4_real-time_face_recog\\models\n",
      "Metagraph file: model-20180402-114759.meta\n",
      "Checkpoint file: model-20180402-114759.ckpt-275\n",
      "WARNING:tensorflow:From C:\\Users\\user\\anaconda3\\envs\\py37_tf\\lib\\site-packages\\tensorflow\\python\\training\\queue_runner_impl.py:391: QueueRunner.__init__ (from tensorflow.python.training.queue_runner_impl) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "To construct input pipelines, use the `tf.data` module.\n",
      "WARNING:tensorflow:From C:\\Users\\user\\anaconda3\\envs\\py37_tf\\lib\\site-packages\\tensorflow\\python\\training\\saver.py:1276: checkpoint_exists (from tensorflow.python.training.checkpoint_management) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use standard file APIs to check for files with this prefix.\n",
      "INFO:tensorflow:Restoring parameters from C:\\Users\\user\\face_recog\\DGEs\\FaceNet\\step4_real-time_face_recog\\models\\model-20180402-114759.ckpt-275\n",
      "Debug enabled\n"
     ]
    }
   ],
   "source": [
    "main(args) # initiate main function"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Here is our final result\n",
    "<br><p style=\"text-align:left;\"><img src=\"images/result.png\"  style=\"width:861px;height:496px;\" align=\"left\">"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
